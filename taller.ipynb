{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Análisis de Sentimientos usando Naive Bayes\n",
    "-----------------------------------------------------------------------------------------\n",
    "\n",
    "El archivo `amazon_cells_labelled.txt` contiene una serie de comentarios sobre productos\n",
    "de la tienda de amazon, los cuales están etiquetados como positivos (=1) o negativos (=0)\n",
    "o indterminados (=NULL). En este taller se construirá un modelo de clasificación usando\n",
    "Naive Bayes para determinar el sentimiento de un comentario.\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pregunta_01():\n",
    "    \"\"\"\n",
    "    Carga de datos.\n",
    "    -------------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    # Lea el archivo `amazon_cells_labelled.tsv` y cree un DataFrame usando pandas.\n",
    "    # Etiquete la primera columna como `msg` y la segunda como `lbl`. Esta función\n",
    "    # retorna el dataframe con las dos columnas.\n",
    "    df = pd.read_csv(\n",
    "        'amazon_cells_labelled.tsv',\n",
    "        sep='\\t',\n",
    "        header=None,\n",
    "        names=[\"msg\", \"lbl\"],\n",
    "    )\n",
    "\n",
    "    # Separe los grupos de mensajes etiquetados y no etiquetados.\n",
    "    df_tagged = df[df[\"lbl\"].notna()]\n",
    "    df_untagged = df[df[\"lbl\"].isna()]\n",
    "\n",
    "    x_tagged = df_tagged[\"msg\"]\n",
    "    y_tagged = df_tagged[\"lbl\"]\n",
    "\n",
    "    x_untagged = df_untagged[\"msg\"]\n",
    "    y_untagged = df_untagged[\"lbl\"]\n",
    "\n",
    "    # Retorne los grupos de mensajes\n",
    "    return x_tagged, y_tagged, x_untagged, y_untagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pregunta_02():\n",
    "    \"\"\"\n",
    "    Preparación de los conjuntos de datos.\n",
    "    -------------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    # Importe train_test_split\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Cargue los datos generados en la pregunta 01.\n",
    "    x_tagged, y_tagged, x_untagged, y_untagged = pregunta_01()\n",
    "\n",
    "    # Divida los datos de entrenamiento y prueba. La semilla del generador de números\n",
    "    # aleatorios es 12345. Use el 10% de patrones para la muestra de prueba.\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x_tagged,\n",
    "        y_tagged,\n",
    "        test_size=0.1,\n",
    "        random_state=12345,\n",
    "    )\n",
    "\n",
    "    # Retorne `X_train`, `X_test`, `y_train` y `y_test`\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pregunta_03():\n",
    "    \"\"\"\n",
    "    Construcción de un analizador de palabras\n",
    "    -------------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    # Importe el stemmer de Porter\n",
    "    # Importe CountVectorizer\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    # Cree un stemeer que use el algoritmo de Porter.\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Cree una instancia del analizador de palabras (build_analyzer)\n",
    "    analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "    # Retorne el analizador de palabras\n",
    "    return lambda x: (stemmer.stem(w) for w in analyzer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pregunta_04():\n",
    "    \"\"\"\n",
    "    Especificación del pipeline y entrenamiento\n",
    "    -------------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    # Importe CountVetorizer\n",
    "    # Importe GridSearchCV\n",
    "    # Importe Pipeline\n",
    "    # Importe BernoulliNB\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "    # Cargue las variables.\n",
    "    x_train, _, y_train, _ = pregunta_02()\n",
    "\n",
    "    # Obtenga el analizador de la pregunta 3.\n",
    "    analyzer = pregunta_03()\n",
    "\n",
    "    # Cree una instancia de CountVectorizer que use el analizador de palabras\n",
    "    # de la pregunta 3. Esta instancia debe retornar una matriz binaria. El\n",
    "    # límite superior para la frecuencia de palabras es del 100% y un límite\n",
    "    # inferior de 5 palabras. Solo deben analizarse palabras conformadas por\n",
    "    # letras.\n",
    "    countVectorizer = CountVectorizer(\n",
    "        analyzer=analyzer,\n",
    "        lowercase=True,\n",
    "        stop_words='english',\n",
    "        token_pattern=r\"\\b\\w\\w+\\b\",\n",
    "        binary=True,\n",
    "        max_df=1.0,\n",
    "        min_df=5,\n",
    "    )\n",
    "    # Cree un pipeline que contenga el CountVectorizer y el modelo de BernoulliNB.\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"countVectorizer\", countVectorizer),\n",
    "            (\"BernoulliNB\", BernoulliNB()),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Defina un diccionario de parámetros para el GridSearchCV. Se deben\n",
    "    # considerar 10 valores entre 0.1 y 1.0 para el parámetro alpha de\n",
    "    # BernoulliNB.\n",
    "    param_grid = {\n",
    "        \"BernoulliNB__alpha\": np.linspace(0.1, 1, 10),\n",
    "    }\n",
    "    # Defina una instancia de GridSearchCV con el pipeline y el diccionario de\n",
    "    # parámetros. Use cv = 5, y \"accuracy\" como métrica de evaluación\n",
    "    gridSearchCV = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring=\"accuracy\",\n",
    "        refit=True,\n",
    "        return_train_score=False,\n",
    "    )\n",
    "\n",
    "    # Búsque la mejor combinación de regresores\n",
    "    gridSearchCV.fit(x_train, y_train)\n",
    "\n",
    "    # Retorne el mejor modelo\n",
    "    return gridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pregunta_05():\n",
    "    \"\"\"\n",
    "    Evaluación del modelo\n",
    "    -------------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    # Importe confusion_matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    # Obtenga el pipeline de la pregunta 3.\n",
    "    gridSearchCV = pregunta_04()\n",
    "\n",
    "    # Cargue las variables.\n",
    "    X_train, X_test, y_train, y_test = pregunta_02()\n",
    "\n",
    "    # Evalúe el pipeline con los datos de entrenamiento usando la matriz de confusion.\n",
    "    cfm_train = confusion_matrix(\n",
    "        y_true=y_train,\n",
    "        y_pred=gridSearchCV.predict(X_train),\n",
    "    )\n",
    "\n",
    "    cfm_test = confusion_matrix(\n",
    "        y_true=y_test,\n",
    "        y_pred=gridSearchCV.predict(X_test),\n",
    "    )\n",
    "\n",
    "\n",
    "    # Retorne la matriz de confusion de entrenamiento y prueba\n",
    "    return cfm_train, cfm_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pregunta_06():\n",
    "    \"\"\"\n",
    "    Pronóstico\n",
    "    -------------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtenga el pipeline de la pregunta 3.\n",
    "    gridSearchCV = pregunta_04()\n",
    "\n",
    "    # Cargue los datos generados en la pregunta 01.\n",
    "    _, _, X_untagged, _ = pregunta_01()\n",
    "\n",
    "    # pronostique la polaridad del sentimiento para los datos\n",
    "    # no etiquetados\n",
    "    y_untagged_pred = gridSearchCV.predict(X_untagged)\n",
    "\n",
    "    # Retorne el vector de predicciones\n",
    "    return y_untagged_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('dsbasics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fdef10015e310e18bb1b03c6e2d154ddadec9c59a1fe30b9a7c0b13350d0b918"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
